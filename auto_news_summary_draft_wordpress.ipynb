{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06966cef",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "504e8bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model that extracts topic lines by deep learning\n",
    "from transformers import pipeline\n",
    "\n",
    "# for web scraping\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib\n",
    "import re\n",
    "from requests_html import HTML\n",
    "from requests_html import HTMLSession\n",
    "\n",
    "# language dectection\n",
    "from langdetect import detect\n",
    "\n",
    "# google translator api\n",
    "import googletrans\n",
    "\n",
    "# auto-posting for wordpress sites\n",
    "from wordpress_xmlrpc import Client, WordPressPost\n",
    "from wordpress_xmlrpc.methods import posts\n",
    "from wordpress_xmlrpc.methods.posts import GetPosts, NewPost, EditPost\n",
    "from wordpress_xmlrpc.methods.users import GetUserInfo\n",
    "\n",
    "# for start and end of loop condition and setting loop interval\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "#turning of ssl for auto-editing Wordpress\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511f0eef",
   "metadata": {},
   "source": [
    "# Required Search Keyword and ID, Pass, Draft Title for Wordpress Site and Loop Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9191dbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide search key word to scrape from google news\n",
    "s_key = \"desired search keywords here \"\n",
    "\n",
    "# wordpress site id and pass\n",
    "wid = \"your wordpress site id\"\n",
    "wpass = \"your wordpress site password\"\n",
    "\n",
    "# Wordpress draft post's title\n",
    "dft_title = \"title of draft you want to edit automatically\"\n",
    "\n",
    "# setting loop end time and interval\n",
    "end_time = 8  # 1 to 24 scale\n",
    "interval_hr= 1 # desired interval in hour\n",
    "interval = interval_hr * 3600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6394d2a",
   "metadata": {},
   "source": [
    "# Functions\n",
    "### 1) Google Search Related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd8593d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_eng(text):\n",
    "    '''\n",
    "    Check if an article element in the article list is written in English\n",
    "    \n",
    "    '''\n",
    "    check_count = 0\n",
    "    for l in text:\n",
    "        try:\n",
    "            if check_count == 0:\n",
    "                lang = detect(l)\n",
    "                check_count += 1\n",
    "                return lang\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d462762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source(url):\n",
    "    \"\"\" \n",
    "    argument: url to scrape.\n",
    "\n",
    "    return:\n",
    "        response (HTTP response from requests_html).\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        session = HTMLSession()\n",
    "        response = session.get(url)\n",
    "        return response\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e376ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_google(query):\n",
    "    '''\n",
    "    Scrapes first page of the search results updated within 1 hour on google news, given\n",
    "    search keyword (query).\n",
    "    '''\n",
    "    query = urllib.parse.quote_plus(query)\n",
    "    # most recent (1hr) 'news' articles on google \n",
    "    response = get_source(\"https://www.google.com/search?q=\" + query + '&newwindow=1&tbm=nws&sxsrf=APq-WBubXX6mz3DlY3jAI6sPYeISvwNsBw:1645516938257&source=lnt&tbs=qdr:h&sa=X&ved=2ahUKEwjujs6k7JL2AhUEDt4KHWzgBc4QpwV6BAgBEBQ&biw=958&bih=999&dpr=1')\n",
    "\n",
    "    links = list(response.html.absolute_links)\n",
    "    google_domains = ('https://www.google.', \n",
    "                      'https://google.', \n",
    "                      'https://webcache.googleusercontent.', \n",
    "                      'http://webcache.googleusercontent.', \n",
    "                      'https://policies.google.',\n",
    "                      'https://support.google.',\n",
    "                      'https://maps.google.')\n",
    "\n",
    "    for url in links[:]:\n",
    "        if url.startswith(google_domains):\n",
    "            links.remove(url)\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1532cb5",
   "metadata": {},
   "source": [
    "### 2) Text Cleaning and Chunking Text for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d41c0082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean(url):\n",
    "    '''\n",
    "    Clean the text from a scraped article from a site, then divide it into a 500 word chunk\n",
    "    to feed it to the transformer summarization model.\n",
    "    \n",
    "    '''\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    results = soup.find_all(['h1','p'])\n",
    "    text = [result.text for result in results]\n",
    "    \n",
    "    index_count = 0\n",
    "    cleaned = []\n",
    "    #initial (impossible) index for later exclusion of unrelated words such as contact info.\n",
    "    ind = 1000000000000\n",
    "    for c in text:\n",
    "        # get index number to remove lines after 'contact info.'\n",
    "        c_index = text.index(c)\n",
    "        # get rid of ad. sentences with email addresses, phone numbers, contact info,\n",
    "        # empty element in the article\n",
    "        if (re.search('[a-z]\\@[a-z]', c)) or (re.search('\\+[0-9][0-9]', c)) or ('contact:' in c) or (c == ''):\n",
    "            continue\n",
    "        elif (re.search('For more information*', c)) or (re.search('for more information*', c)):\n",
    "            # get the index where site's contact info. starts\n",
    "            ind = c_index\n",
    "            # remove(not append) lines having contact info.\n",
    "        elif c_index > ind:\n",
    "                continue     \n",
    "        else:\n",
    "            # get rid of \\n in the article\n",
    "            new_c = c.strip('\\n')\n",
    "            # get rid of '' mark within a sentence\n",
    "            new_c = new_c.replace(\"''\", \"\")\n",
    "            # remove \\xa0\n",
    "            new_c = new_c.replace(\"\\xa0\", \"\")\n",
    "            # append the striped list element to a new list\n",
    "            cleaned.append(new_c)\n",
    "        index_count += 1\n",
    "    \n",
    "    article = ' '.join(cleaned)\n",
    "    \n",
    "    article = article.replace('.', '.<eos>')\n",
    "    article = article.replace('!', '!<eos>')\n",
    "    article = article.replace('?', '?<eos>')\n",
    "    sentences = article.split('<eos>')\n",
    "    \n",
    "    max_chunk = 500\n",
    "    current_chunk = 0\n",
    "    chunks = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if len(chunks) == current_chunk +1:\n",
    "            if len(chunks[current_chunk]) + len(sentence.split(' ')) <= max_chunk:\n",
    "                chunks[current_chunk].extend(sentence.split(' '))\n",
    "            else:\n",
    "                current_chunk += 1\n",
    "                chunks.append(sentence.split(' '))\n",
    "        else:\n",
    "            chunks.append(sentence.split(' '))\n",
    "            \n",
    "    for chunk_id in range(len(chunks)):\n",
    "        chunks[chunk_id] = ' '.join(chunks[chunk_id])\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb358669",
   "metadata": {},
   "source": [
    "### 3) Auto-Draft-Editing on Wordpress Site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51e3cb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_draft(new_content):\n",
    "    client = Client('https://idknn.com/xmlrpc.php', wid, wpass)\n",
    "    draft_posts = client.call(posts.GetPosts({'post_status': 'draft'}))\n",
    "    for dp in draft_posts:\n",
    "        if dp.title == dft_title:\n",
    "            prev_content = dp.content\n",
    "            dp.title = dft_title\n",
    "            dp.content = prev_content + \"\\n\\n\" + new_content\n",
    "            dp.thumbnail = dp.id\n",
    "            client.call(posts.EditPost(dp.id,dp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe88879",
   "metadata": {},
   "source": [
    "# Text Summarizer Model from Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "422a4a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline('summarization')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79665a7",
   "metadata": {},
   "source": [
    "# Summary Generation and Auto-Draft_Editing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875772d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop until end_time\n",
    "while datetime.now().hour < end_time:\n",
    "    \n",
    "    # search keyword to search on google news\n",
    "    links = scrape_google(s_key)\n",
    "\n",
    "    # remove sites that block scraper.\n",
    "    valid_links = []\n",
    "\n",
    "    # get the list of sites that blocks scraper for the future use\n",
    "    block_sites = []\n",
    "\n",
    "    for link in links:\n",
    "\n",
    "        # exclude nasdaq site for it makes bot down\n",
    "        temp = link.split('.')\n",
    "        if 'nasdaq' in temp:\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "\n",
    "            # try and see if there's a site blocking scraper\n",
    "            try:\n",
    "                r = requests.get(link)\n",
    "\n",
    "                # for checking links that are not in English\n",
    "                soup = BeautifulSoup(r.text, 'html.parser')\n",
    "                results = soup.find_all(['h1','p'])\n",
    "                text = [result.text for result in results]\n",
    "\n",
    "                # for checking links that block scraper\n",
    "                check = BeautifulSoup(r.text, 'html.parser').get_text(strip=True)\n",
    "                temp = link.split('.')\n",
    "\n",
    "                # remove links that contain error messages or non-topic-related text\n",
    "                if (re.search(\"Error\",check)) or (check == '') or (len(check) < 1500):\n",
    "                    continue\n",
    "                # filtering out articles not in English\n",
    "                if check_eng(text) != 'en':\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "                    valid_links.append(link)\n",
    "\n",
    "            except:\n",
    "                if link not in block_sites:\n",
    "                    block_sites.append(link)\n",
    "\n",
    "    # List of previous articles already got, to be excluded\n",
    "    previous_articles = []\n",
    "    # new articles to be worked with\n",
    "    article_list = []\n",
    "\n",
    "    for atc in valid_links:\n",
    "        if atc not in previous_articles:\n",
    "            previous_articles.append(atc)\n",
    "            article_list.append(atc)\n",
    "\n",
    "    # get summary sentences from each scraped valid articles\n",
    "    articles_done = {}\n",
    "    for url in article_list:\n",
    "        chunks = get_clean(url)\n",
    "        res = summarizer(chunks, max_length=150, min_length=10, do_sample=False)\n",
    "        articles_done[url] = res[0]['summary_text']\n",
    "\n",
    "    # translate each summary into Korean, key=site address, val=translated text\n",
    "    # then auto-add it on a Wordpress site draft\n",
    "    for k, v in articles_done.items():\n",
    "        translator = googletrans.Translator() \n",
    "        result = translator.translate(articles_done[k], dest='ko') \n",
    "        translated = result.text\n",
    "        new_content = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \"\\n\" + \"Original:  \" + v + \"\\n\\n\" + \"Translated:  \" + translated + \"\\n\\n\" + '<a href=\"{}\">{}</a>'.format(k,k) + \"\\n\\n\\n\"\n",
    "        edit_draft(new_content)\n",
    "    \n",
    "    time.sleep(interval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
